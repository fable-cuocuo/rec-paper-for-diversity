# 推荐系统中的多兴趣学习方法深度研究报告

过去六年间,多兴趣学习已从实验性技术演变为工业级推荐系统的核心架构。本报告深入分析了2019-2025年间10篇里程碑式论文及相关工作,揭示了该领域从简单的兴趣提取到复杂的时序感知、层次化、对话式多兴趣建模的演进路径。核心发现表明:兴趣趋同问题可以通过多种机制有效解决(隐式路由、显式对比学习、统计分离),但过度的正交约束确实会破坏相关兴趣之间的自然联系,而最新的方法正在通过理论基础(谱聚类)、统计方法(直方图)和层次化表示来平衡这一权衡。

## 多兴趣学习的演进历程与核心突破

多兴趣学习的发展经历了三个关键阶段。**第一阶段(2019-2020)**以MIND为代表,首次将胶囊网络引入推荐系统,通过动态路由机制从用户行为序列中提取K个兴趣向量,并在淘宝实现了十亿级用户的工业化部署。ComiRec紧随其后,提出了可控聚合框架,让系统能够灵活平衡准确性与多样性。这一阶段的方法主要依赖架构设计的隐式偏置来维持兴趣分离。

**第二阶段(2021-2023)**标志着显式多样性优化的兴起。MDSR率先在损失函数中加入Intent-aware Diversity Promoting(IDP)项,显式地惩罚兴趣相似性。MIP则提出了聚类增强的多头注意力机制,通过学习个性化的兴趣权重来捕捉用户偏好的动态变化,关键创新是训练后可调整兴趣数量K而无需重新训练。REMI从训练优化角度重新审视多兴趣学习,发现了路由坍塌(routing collapse)这一根本问题——每个兴趣向量会退化为只表达单个物品的信息,通过路由正则化(RR)和兴趣感知的困难负样本挖掘(IHN)实现了6-18%的性能提升。

**第三阶段(2024-2025)**呈现出理论化、统一化、场景化的三大趋势。DisMir提供了首个理论保证,证明基于全局物品划分的解耦方法与谱聚类的内在联系,通过马尔可夫随机场(MRF)采样高效优化,同时缓解item-level和facet-level两类坍塌问题。Trinity提出了革命性的统一框架,将多兴趣、长尾兴趣、长期兴趣视为同一问题(兴趣遗忘)的三个维度,通过实时聚类系统和统计兴趣直方图在抖音实现了数十亿级日推荐。CoMoRec将多兴趣学习扩展到对话式推荐场景,融合上下文、图和评论三个信息源,通过多兴趣对比模块显式优化多样性。HORAE则首次深度整合时序动态与多兴趣预训练,建模兴趣如何沿时间线演化,为2025年的研究方向树立了新标杆。

## 兴趣向量提取机制的技术谱系

### 动态路由方法(胶囊网络范式)

MIND开创的动态路由机制通过Behavior-to-Interest(B2I)路由算法实现兴趣提取。核心流程包括:(1)将行为嵌入{e_i}通过共享双线性矩阵S ∈ R^(d×d)映射,(2)计算路由权重w_ij = softmax(b_ij),其中b_ij = u_j^T S e_i,(3)聚合候选向量z_j = Σ_i w_ij S e_i,(4)通过squash激活函数归一化:u_j = (||z_j||^2 / (1 + ||z_j||^2)) · (z_j / ||z_j||),(5)迭代更新路由logits:b_ij ← b_ij + u_j^T S e_i。与原始胶囊网络的关键差异在于MIND使用单一共享矩阵S而非每对行为-兴趣都有独立参数,这对处理变长序列至关重要。

这种方法的避免趋同机制非常巧妙:**通过随机初始化路由logits b_ij ~ N(0, σ^2)而非全零初始化**。这类似K-means++的初始化策略,确保不同兴趣胶囊从不同起点开始迭代,实验证明在σ取值0.1到5之间都表现出鲁棒性。ComiRec-DR沿用了这一机制,但将学习率特别设置为0.005以稳定训练。动态路由的本质是软聚类,通过迭代优化自然地将相似行为分配给同一兴趣,同时保持不同兴趣的分离,**这种隐式方式能够保留相关兴趣之间的自然联系**,不会强行拆散本应协同的兴趣模式。

### 自注意力与多头注意力方法

ComiRec-SA提出了更高效的替代方案:多头自注意力。K个注意力头分别捕获K个兴趣,每个头通过Query-Key-Value机制独立计算:Interest_k = Attention(Q_k, K_k, V_k)。这种方法的计算复杂度从O(K × |I_u| × 3)路由迭代降至O(|I_u|^2 × d)单次前向传播,在ComiRec的实验中性能反而优于动态路由。关键优势在于Transformer的多头机制天然适合学习不同模式,不同头的初始化自然导致兴趣分化。

MIP进一步创新,提出**聚类增强的个性化自注意力**。与ComiRec/PinText2使用共享query向量不同,MIP的query、key、value都从用户自身交互物品投影生成,避免了全局流行类别偏置。公式为:s^h_{i,j} = [(W^h_q e_j)^T · (W^h_r e_i)] / √d,其中e_j包含物品嵌入、时间编码τ(t_j)和位置编码ρ(j)。计算注意力输出后,MIP对{φ_j}应用聚类算法(默认Ward层次聚类,也支持K-means、谱聚类、BIRCH、DBSCAN),选择每个簇的最后一个物品作为代表。这种后置聚类的优势是**训练后可灵活调整K值**,k∈[5,10]之间AUC差异小于0.5%,为存储-计算权衡提供了极大灵活性。

### 对比学习驱动的解耦方法

DisMir代表了理论驱动的新范式。核心洞察是基于**全局物品共现概率**进行解耦,并证明其与谱聚类的理论联系。DisMir通过马尔可夫随机场(MRF)建模物品之间的联合购买关系,从两个独立MRF中采样小规模子图,将问题近似为交叉熵损失,通过对比学习框架优化。具体而言,将同一兴趣簇内的物品作为正样本,不同簇的物品作为负样本,拉近簇内物品嵌入,推远簇间物品嵌入。

这种方法的革命性在于:**在全局层面进行物品划分,而非针对每个用户单独提取兴趣**。全局结构捕获了整个数据集的共现模式,然后用户的兴趣向量根据其行为在这些预定义簇中进行路由。这既保证了不同兴趣的discriminative capacity,又维持了全局一致性。DisMir同时解决了item-level collapse(不同兴趣表示趋同)和facet-level collapse(多面兴趣无法维持独特性)。作为插件式框架,DisMir可增强MIND、ComiRec等现有方法,实验证明在三个真实数据集上显著优于基线。

CoMoRec在对话式推荐场景引入**多兴趣对比模块**,通过兴趣级别的对比学习确保不同兴趣表示保持独特性。与DisMir的全局物品级对比不同,CoMoRec的对比发生在用户表示层面:为每个兴趣向量创建正负样本对,通过自监督学习塑造兴趣空间。这种方法特别适合对话系统,因为对话中的用户意图会动态变化,需要实时调整兴趣激活模式。

### 统计与直方图方法

Trinity提出了完全不同的思路,避开了学习嵌入的范式,转而采用**统计兴趣直方图**。首先通过离线聚类系统将物品投影到可枚举的簇(通常100-1000个簇),然后为每个用户-簇对计算交互统计特征:簇交互频率、交互新鲜度、趋势(增长/衰减)、欠交付分数。数学表示为:H_u(c, t)表示用户u在时间窗口t内与簇c的交互直方图。

关键创新是识别**欠交付主题**(underdelivered themes):历史上有强交互(H_u(c, t_historical) > threshold)但近期曝光不足(H_u(c, t_recent) < threshold_recent)的簇。这些簇代表被遗忘的兴趣,需要重新平衡。统计方法的巨大优势在于:**计算效率极高**(每用户O(K)复杂度)、**解释性强**(直接看出哪些兴趣被忽视)、**避免嵌入坍塌**(统计量独立计算,不联合优化)。Trinity在抖音的成功部署证明了这种方法在数十亿级用户场景的有效性。

## 兴趣趋同问题的破解之道

### 隐式方法:架构归纳偏置

MIND和ComiRec-DR依赖动态路由的软聚类特性,无显式多样性约束。优势是**保留相关兴趣的自然关系**——如果用户确实对相关领域感兴趣(如不同品牌的篮球鞋),这些兴趣可以自然地有部分重叠而不被强制分离。劣势是无多样性保证,某些情况下可能坍塌到相似表示。

ComiRec-SA的多头注意力通过不同头的独立初始化实现隐式分化。每个头学习不同的Query-Key映射模式,自然捕获不同关注点。实验证明这种隐式机制在多数情况下足够有效,且训练更简单、收敛更快。

### 显式方法:多样性损失

MDSR/IDSR引入Intent-aware Diversity Promoting(IDP)损失,是首个使用显式多样性约束的方法。总损失为L = L_accuracy + λ·L_diversity,其中多样性项包含意图分离、覆盖度等组件,鼓励不同意图向量保持正交或距离。这保证了多样性,但引入了关键权衡:**λ参数的调优至关重要**。过高的λ会人为拆散相关兴趣,例如对篮球鞋感兴趣的用户,其"Nike篮球鞋"和"Adidas篮球鞋"两个意图本应有关联,但强多样性约束会将它们强制推远,破坏了自然的兴趣结构。过低的λ则约束不足,可能无法防止坍塌。

CoMoRec的多目标优化L_total = λ_1·L_rec + λ_2·L_response + λ_3·L_diversity + λ_4·L_contrast也面临类似挑战,但在对话系统中多样性是显式需求(避免过滤气泡、马太效应),因此这种权衡是合理的。实验显示CoMoRec在维持准确性的同时实现了多样性指标10-20%的相对提升。

### 训练优化方法:REMI的双重策略

REMI识别了多兴趣学习中的两个根本训练问题,提出了不修改架构的解决方案:

**路由正则化(RR)**:解决路由坍塌,即每个兴趣只聚合单个物品信息。损失为L_RR = λ Σ_i ||r_i - r̄||^2,惩罚物品i的路由分布r_i偏离平均分布r̄。这鼓励每个兴趣从多个物品聚合信息,保持丰富性。实验中λ取值对数据集敏感:Books为100,Gowalla为1,Rocket为0.1,需要验证集调优。

**兴趣感知困难负样本挖掘(IHN)**:多兴趣设置创造了指数级更多简单负样本,均匀采样训练效率低。IHN提出理想采样分布p(j) ∝ exp(β·max_k(z_k·v_j)),优先采样接近用户某个兴趣但未被点击的物品。温度参数β控制难度。这提升了判别性学习,帮助模型学习更清晰的决策边界。

REMI的优雅之处在于作为**通用框架可增强任何现有多兴趣方法**,实现简单(不到50行代码),训练开销小(15%增量),推理无开销,且在Books、Gowalla、Rocket数据集上分别实现了18.1%、12.4%、6.5%的Recall@50提升。消融研究显示IHN和RR的协同效应大于各自独立贡献之和。

### 理论基础与全局方法:DisMir和Trinity

DisMir的理论贡献在于连接物品划分与谱聚类,提供了形式化的有效性保证。通过在全局层面优化物品簇分配(基于共现可能性),然后在用户层面应用多任务学习框架无缝对齐物品划分学习与多兴趣建模,DisMir实现了item-level和facet-level的双重解耦。这种全局视角**既保证了多样性(簇间差异),又保留了相关性(簇内物品自然相关)**。

Trinity的统计直方图方法从根本上避免了嵌入坍塌问题,因为统计量是独立计算的,不存在梯度耦合。不同时间窗口(短期、中期、长期)的直方图自然捕获兴趣演化,而欠交付检测机制动态平衡了探索与利用。这种方法在面对新兴热点时保持稳定,不会过度拟合短期趋势,体现了工业系统的成熟性。

## 兴趣激活与选择的精细机制

### 训练时选择:标签感知注意力

MIND在训练阶段使用label-aware attention:查询为目标物品嵌入e_i,键值为兴趣胶囊V_u = (v_u^1,...,v_u^K),注意力输出v_u = V_u · softmax(pow(V_u^T e_i, p))。幂参数p控制注意力分布的尖锐度:p=0为均匀注意力(性能最差),p≥1为比例注意力,p→∞逼近硬注意力(性能最佳)。这种软注意力机制在训练时提供了平滑梯度,有助于收敛。

但MIND存在训练-服务不一致:训练用软注意力,服务用最大池化f_score(V_u, e_i) = max_{1≤k≤K} e_i^T v_u^k。这种差异可能导致次优解,因为模型优化的目标(加权和)与实际使用的目标(最大值)不匹配。

### 服务时选择:最大池化与加权聚合

ComiRec提出了**可控聚合模块**,允许用户或系统控制准确性-多样性权衡。不同于MIND的硬性最大池化,ComiRec在候选检索阶段为每个兴趣独立检索,然后通过可控因子在聚合排序时平衡相关性与多样性。这种设计承认了**用户多样性偏好的异质性**:探索型用户(explorers)倾向高多样性推荐,专注型用户(exploiters)倾向一致性推荐。

MIP的创新在于**学习个性化兴趣权重**w_j,而非启发式指数衰减。权重计算为w_j = FFN([z_j; 1_{C_1∈L_j}·τ_1;...; 1_{C_l∈L_j}·τ_l]),其中FFN使用sigmoid激活捕获时序参与模式,输出经softplus归一化确保正值。这些权重同时捕获簇亲和力和时间衰减,为每个用户个性化而非全局统一。预测采用y = max{w_j Linear(z_j·p)}^k_{j=1},取所有兴趣的加权最大值(如果任一兴趣匹配则用户参与)。

### 动态上下文感知激活

CoMoRec在对话式推荐中实现了**对话驱动的兴趣选择**。分析对话上下文识别当前推荐轮次的相关兴趣,使用注意力机制基于对话状态对不同兴趣面(上下文、图、评论)进行加权。动态兴趣组合为α_context·v_context + α_graph·v_graph + α_review·v_review,其中注意力权重α根据对话流自适应:早期轮次可能强调上下文,后续轮次整合图/评论信号。这种动态性对多轮对话至关重要,确保推荐随对话演进而调整。

HORAE引入了**时序感知的兴趣激活**。通过建模三种时间信息(相对位置、物品间时间间隔、绝对时间上下文),每个兴趣胶囊根据时间上下文动态激活或去激活。序列精炼机制学习兴趣如何沿时间线微妙变化和转移,捕获兴趣收敛/发散模式,提供更精准的动态用户偏好表示。

## 用户多样性偏好的个性化建模

### 显式多样性参数

ComiRec的可控聚合是最直接的方法,通过可调参数让用户明确表达多样性偏好或让系统基于历史行为推断。但这要求用户主动设置或系统准确估计,实践中可能面临挑战。

### 学习的偏好权重

MIP通过学习兴趣权重隐式建模多样性偏好。对某些兴趣权重高的用户表现出强偏好,对多个兴趣权重均衡的用户则偏好多样化推荐。这种方法的优势是**从数据中自动学习**,无需显式标注或用户输入。两阶段训练策略(Phase 1:等权重,Phase 2:学习权重)确保了稳定收敛。

### 用户中心的多样性

CoMoRec明确提出**用户中心方法**,认识到不同用户有不同多样性偏好。通过个性化多样性级别而非统一约束,模型基于用户历史行为模式学习其偏好谱:广泛的探索型用户vs狭窄的专注型用户。这与one-size-fits-all的多样性方法形成鲜明对比,体现了现代推荐系统的精细化趋势。

### 群体对比学习

C2AL(Cohort-Contrastive Auxiliary Learning, 2024)从另一个角度解决用户异质性:缓解大规模系统中的表示偏差。多数群体(majority cohort)倾向主导模型学习,少数群体代表性不足。C2AL通过注意力机制学习更密集的权重分布,平衡不同群体的表示,确保多样化用户的偏好都得到充分建模。

## 训练目标与损失函数的精巧设计

### 单一目标:采样softmax

MIND使用标准采样softmax损失Pr(i|u) = exp(v_u^T e_i) / Σ_j∈I exp(v_u^T e_j),在数十亿物品规模下通过负采样实现可扩展性。优化器为Adam,无辅助损失,简洁高效。但这种单一目标可能导致过度拟合流行物品。

### Pointwise vs Pairwise vs Listwise

**Pointwise**:预测单个物品的点击概率,如MIND、MIP的交叉熵/负对数似然损失。优势是简单直接,劣势是忽略了物品间相对顺序。

**Pairwise**:优化物品对的相对顺序,如BPR(Bayesian Personalized Ranking)损失。许多方法隐式使用pairwise思想,通过正负样本对比学习。REMI的IHN本质上是pairwise方法,通过困难负样本增强判别性。

**Listwise**:考虑整个推荐列表的全局最优性,如LambdaRank、ListMLE。在多兴趣学习中较少直接使用,但多样性约束(如DPP)有listwise flavor。

### 多目标联合优化

MDSR/IDSR率先提出双目标:L = L_accuracy + λ·L_diversity。CoMoRec扩展为四目标:L_total = λ_1·L_rec + λ_2·L_response + λ_3·L_diversity + λ_4·L_contrast,平衡推荐准确性、响应生成、多样性和对比学习。这种多任务学习框架的挑战在于**超参数调优**(多个λ)和**梯度冲突**(不同目标可能相互对立)。

REMI提供了更优雅的方案:L = L_base + β·L_IHN + λ·L_RR,其中基础损失保留原方法,IHN和RR作为增强项。实验显示β=0.1-10、λ=0.1-100范围内性能鲁棒,且两项协同效应显著。

### 对比学习目标

DisMir采用对比框架近似MRF优化,使用交叉熵损失。CoMoRec的兴趣级对比损失确保不同兴趣表示保持区分性。HORAE在Miracle基础上增加了**时序对比目标**,对齐不同时间上下文的兴趣表示,确保兴趣演化模式的时序一致性。

对比学习的核心是正负样本对的构建:**正样本**通常是同一兴趣/簇的不同视图或物品,**负样本**是不同兴趣/簇的物品。InfoNCE损失L = -log[exp(sim(z_i, z_i^+)/τ) / Σ_j exp(sim(z_i, z_j)/τ)]通过温度τ控制分布尖锐度。自监督预训练(如HORAE)依赖对比学习学习通用兴趣模式,这些模式可跨域迁移。

### 预训练与微调范式

HORAE代表了最新的预训练-微调范式。预训练阶段在大规模跨域数据上学习通用时序多兴趣模式,使用兴趣级对比学习、时序一致性损失、next-item预测的多任务组合。微调阶段在目标域上高效适配,保留时序多兴趣结构的同时专门化到特定领域。这对数据稀疏场景(冷启动、长尾)特别有效,利用预训练的通用知识弥补目标域数据不足。

## 序列编码器的架构选择

### RNN/GRU的衰落

早期序列推荐广泛使用RNN/GRU(如GRU4Rec, 2015)。优势是顺序处理、紧凑表示。劣势明显:**顺序计算导致训练慢、难以捕获长期依赖、梯度消失问题**。到2022年,Transformer模型在几乎所有基准上都超越了循环模型,RNN/GRU逐渐被边缘化。

### Transformer/自注意力的主导

SASRec(2018)和BERT4Rec(2019)确立了自注意力在序列推荐的主导地位。优势包括:**并行训练、直接建模长期依赖、更好的性能**。SASRec使用单向Transformer解码器(left-to-right),BERT4Rec使用双向Transformer编码器(Cloze任务),后者理论上更强但RecSys 2022的研究发现不同实现间结果不一致,可重复性存在问题。

多兴趣学习广泛采用Transformer架构:MIP使用多头自注意力(H=8头,d_model=32-64),DisMir基于Transformer序列模型,HORAE扩展Transformer加入时序位置嵌入(可能使用RoPE - Rotary Position Embeddings)。

### 时序增强编码器

TiSASRec(2020)在自注意力中整合时间间隔,学习时间感知的位置嵌入,捕获用户偏好的时间漂移。MEANTIME(2020)采用多种时间嵌入方案,编码绝对和相对时间戳,证明多样化时间信号的重要性。

HORAE(2025)代表时序建模的最新高度,联合考虑三种时间信息:相对位置信息、物品间时间间隔、绝对时间上下文。每个兴趣胶囊enriched with时间演化信号,序列精炼机制建模兴趣如何沿时间线变化转移。这种精细的时序感知使得HORAE能够捕获兴趣的涌现、衰退、周期性/非周期性模式。

### 高效Transformer与长序列建模

Transformer的二次复杂度O(n^2)成为长序列瓶颈。解决方案包括:**线性注意力**(将复杂度降至O(n))、**稀疏注意力**(只关注子集位置)、**分解注意力**(分解注意力矩阵)、**状态空间模型**(Mamba架构,LC-Mamba 2024)、**层次化方法**(多级序列建模)、**检索增强**(存储并检索长历史)。

### 专家混合(MoE)架构

HM4SR(WWW 2025)提出层次化时间感知MoE:Interactive MoE提取兴趣相关的多模态信息,Temporal MoE捕获动态兴趣与显式时间嵌入,不同专家处理不同兴趣或时间模式。这种架构在保持模型容量的同时提升效率,因为每次只激活部分专家。

## 层次化兴趣表示的探索

### 粗细粒度的自然层次

Trinity是层次化兴趣的典范。**粗粒度**:物品聚类到100-1000个簇,代表高层兴趣类别(如运动、娱乐、教育)。**细粒度**:簇内具体物品嵌入,实现精准匹配的同时维持效率。统计直方图在粗粒度层级计算(簇级统计),检索在细粒度层级执行(物品级匹配)。这种设计在可解释性(清晰的兴趣主题)和计算效率之间达成了优雅平衡。

### 双尺度Transformer

HEML(Hypergraph-Enhanced Multi-interest Learning, 2024)采用双尺度Transformer:粗粒度级别捕获时间演化的一般兴趣,细粒度级别建模序列模式的具体兴趣。多兴趣分解在这两个尺度上进行,全局多阶多行为依赖通过超图增强学习。这种双尺度方法既捕获宏观趋势又保留微观细节。

### Matryoshka表示学习

MRL(Matryoshka Representation Learning, 2024)将嵌入重构为增量维度的嵌套向量空间,显式表示层次化用户偏好和物品特征。低维表示捕获粗粒度特性,高维表示增加细粒度细节。这种嵌套结构天然支持层次化兴趣:前k维可能表示主要兴趣类别,后续维度细化为子类别。

### 双曲空间层次化

HARec(2024)利用双曲空间的天然层次化特性。树结构表示中,深度搜索对应利用(exploitation),广度搜索对应探索(exploration)。层次化感知的图-LLM对齐机制在双曲几何中学习用户和物品嵌入,效用提升5.49%,多样性提升11.39%,证明几何归纳偏置的威力。

### 语义ID的层次化

Semantic ID(Singh et al., 2023; Rajput et al., 2024)从层次化簇的语义相似性派生物品ID。基于前缀n-gram的方法创建层次化token:ID的前几个token编码粗类别,后续token细化到具体物品。这种稳定的ID空间对嵌入学习更友好,且天然包含层次化信息,对多兴趣学习有潜在价值。

## 兴趣趋同与相关性的哲学权衡

这是多兴趣学习最深刻的理论问题:**如何区分应避免的趋同与应保留的相关性**?

### 破坏相关性的风险

MDSR/IDSR的显式多样性损失存在明确风险。考虑一个真实案例:用户对篮球鞋有浓厚兴趣,历史购买了Nike、Adidas、Under Armour多个品牌。这些兴趣自然相关(都是篮球鞋),在嵌入空间中应该较近。但如果λ设置过高,IDP损失会强制推远这些表示,破坏了"篮球鞋"这一元兴趣下的自然变体。结果是模型可能推荐完全不相关的物品(如足球鞋或跑鞋)来满足多样性约束,但这违背了用户真实偏好。

类似地,对电影的兴趣可能涵盖"诺兰导演的科幻片"和"其他科幻片",这两个兴趣有重叠但不完全相同。显式正交约束可能过度分离它们,导致推荐要么全是诺兰电影,要么全是非诺兰科幻,而失去了两者的合理混合。

### 保留相关性的方法

**动态路由的软聚类**(MIND, ComiRec-DR):通过迭代优化自然形成聚类,相关物品分配给同一兴趣或相邻兴趣,不强制正交。实验证明这种隐式方法在多数情况下足够有效,且保留了语义连贯性。

**全局物品划分**(DisMir):基于共现概率的谱聚类理论上是最优的,因为它反映了真实用户行为中的物品关联。同一簇内的物品自然相关(经常被共同购买),不同簇之间才需要区分。这种全局视角比局部per-user约束更合理。

**统计直方图**(Trinity):根本避开了嵌入空间的相关性问题。统计量独立计算,不存在梯度耦合导致的相互推拉。欠交付检测基于时间窗口内的交互模式,自然反映了兴趣的真实状态。

**学习的权重**(MIP):通过个性化权重而非硬性约束,模型自动学习哪些兴趣应该强(高权重)、哪些应该弱(低权重)。如果用户确实只对篮球鞋感兴趣,多个相关兴趣簇可以都有高权重,不会被人为抑制。

### 理论指导的平衡

DisMir的理论贡献提供了形式化指导:基于谱聚类的物品划分是在全局簇间分离度和簇内紧密度之间的最优权衡。这个理论框架表明,**应该在全局层面而非用户层面进行多样性优化**,用户兴趣只是从这些预定义簇中进行选择和组合。

HORAE的时序建模提供了另一个视角:兴趣的相关性是动态的。某些兴趣在特定时期相关(如夏季的游泳和防晒),在其他时期独立。时序感知的兴趣演化建模能够捕获这种动态相关性,而不是强加静态约束。

### 实践建议

1. **优先使用隐式方法**(动态路由、多头注意力)作为baseline,它们通常足够有效且不破坏自然结构。

2. **显式约束需谨慎调优**。如果使用多样性损失,从小λ开始,监控准确性-多样性trade-off曲线,选择拐点。

3. **结合全局与局部**。先进行全局物品聚类/划分(如DisMir),再在用户层面学习兴趣分布,比纯局部方法更principled。

4. **时序视角**。如果有时间戳数据,考虑兴趣随时间的动态变化,而非静态多样性约束。

5. **用户异质性**。不同用户有不同多样性需求,个性化的多样性水平优于全局参数。

6. **A/B测试验证**。最终通过在线实验验证用户满意度,而非仅依赖离线指标。

## 前沿趋势与未来方向

### 大语言模型的整合

2024-2025年出现了LLM与推荐系统融合的趋势。LLM提供丰富的语义理解,可以为兴趣向量赋予可解释的语义标签。HARec的层次化感知图-LLM对齐、Semantic ID的语义派生都体现了这一方向。未来可期待LLM直接生成兴趣描述("这个兴趣关于现代科幻小说,特别是太空探索主题"),提升可解释性。

### 推理时计算

ReaRec(2025)显示推理时多步推理能实现30-50%性能提升。对多兴趣学习,这意味着不仅在训练时优化兴趣提取,在推理时也可以动态调整兴趣激活、权重、组合策略,根据实时上下文进行个性化计算。

### 基础模型范式

类似NLP的LLM,推荐系统正朝向通用基础模型发展。HORAE、UniSRec、PrepRec等预训练模型展示了跨域迁移的潜力。未来的多兴趣基础模型可能在海量数据上预训练通用兴趣模式,然后zero-shot或few-shot适配到新域,大幅降低数据需求。

### 因果多兴趣学习

当前方法主要基于相关性,但因果理解更根本。理解兴趣之间的因果关系(如对导演的兴趣导致对特定类型电影的兴趣),可以更准确地建模兴趣结构,预测兴趣演化,做出反事实推理。

### 多模态融合

MISSRec、HM4SR已探索多模态多兴趣学习,整合文本、图像、视频、音频。未来方向是更深度的模态融合:不同模态可能揭示不同兴趣面,视觉兴趣vs文本兴趣vs音频兴趣的协同建模。

### 超长序列建模

现有方法通常处理50-500长度的序列,但用户完整生命周期可能有数百万交互。建模整个生命周期的兴趣演化(ultra-long sequence)需要新架构:层次化记忆网络、检索增强生成、压缩表示、状态空间模型等。

### 公平性与去偏

C2AL展示了群体对比学习缓解代表性偏差。未来多兴趣学习需要更多考虑公平性:不同人口群体的兴趣建模是否公平?长尾兴趣(小众爱好)是否被充分表示?流行偏差如何在多兴趣框架下缓解?

### 实时连续学习

Trinity的实时聚类系统、增量直方图更新展示了在线学习的可能。未来系统需要在推荐服务中持续学习用户兴趣变化,而非定期重训练。这要求高效的增量学习算法、快速收敛、稳定性保证。

## 工业实践的宝贵经验

阿里巴巴(淘宝/天猫)的MIND和ComiRec部署证明了多兴趣学习在十亿级用户场景的可行性:**预计算用户兴趣向量,使用Faiss等ANN检索,实现低于15ms的延迟**。关键是离线计算(兴趣提取)与在线服务(ANN检索)的分离,计算复杂度转移到离线阶段。

字节跳动(抖音)的Trinity展示了另一种工业哲学:**统计方法可能比神经网络更适合超大规模系统**。直方图计算高效、可解释、易维护,增量更新成本低,这些工程优势在实际生产中可能超过神经方法的精度提升。

小红书的GemiRec、RimiRec采用层次化多兴趣系统,结合了粗粒度的兴趣类别和细粒度的物品匹配,在中等规模(千万-亿级)上取得了良好效果。

这些工业案例的共同启示:**简洁性、可解释性、工程友好性与算法精度同样重要**。学术界追求SOTA的同时,工业界关注延迟、吞吐、可维护性、A/B测试成本。成功的多兴趣系统需要在这些维度间找到平衡。

## 结论与展望

从2019年MIND的开创性工作到2025年HORAE的时序感知预训练,多兴趣学习已从边缘技术成长为推荐系统的核心范式。核心进展包括:(1)从隐式到显式的多样性优化,(2)从静态到动态的兴趣建模,(3)从局部到全局的优化视角,(4)从单一到层次化的表示结构,(5)从监督学习到自监督预训练范式。

兴趣趋同问题有多种解决方案,各有权衡:动态路由和多头注意力的隐式方法保留自然相关性但无多样性保证;显式多样性损失提供强保证但可能过度分离相关兴趣;REMI的训练优化、DisMir的理论方法、Trinity的统计方法代表了不同哲学路线,都在各自场景下有效。

关键洞察是:**兴趣趋同与兴趣相关性是动态平衡,而非二元对立**。最佳实践是在全局层面(基于共现、谱聚类、统计模式)进行物品组织,在用户层面进行软分配,结合时序动态和个性化权重,避免强硬约束破坏自然结构。层次化表示(粗粒度类别+细粒度物品)提供了优雅的解决方案,既保证多样性又维持相关性。

未来的多兴趣学习将更加智能(LLM整合)、高效(推理时计算、基础模型)、公平(去偏、小众兴趣保护)、动态(实时学习、超长序列)。理论与实践的结合,学术创新与工业验证的闭环,将推动这一领域持续进步,最终实现真正理解并满足用户多样化偏好的智能推荐系统。